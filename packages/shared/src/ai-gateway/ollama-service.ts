/**
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * ğŸ¦™ Ollama Service - Ultimate Edition
 * â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 * 
 * Ø®Ø¯Ù…Ø© Ollama Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ‚Ø¯Ù…Ø§Ù‹ (Ù…Ø¬Ø§Ù†ÙŠØ© 100%)
 * 
 * Ø§Ù„Ù…ÙŠØ²Ø§Øª:
 * âœ… Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø¹Ø§Ø¯ÙŠØ© ÙˆØ¨Ø§Ù„Ø£Ø¯ÙˆØ§Øª
 * âœ… Ø¥Ø¯Ø§Ø±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙˆØ§Ù„Ø³ÙŠØ§Ù‚
 * âœ… ØªØªØ¨Ø¹ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø£Ø¯Ø§Ø¡ (Ø¨Ø¯ÙˆÙ† ØªÙƒÙ„ÙØ©!)
 * âœ… ØªØ­Ø³ÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø£Ø¯ÙˆØ§Øª
 * âœ… Ù†Ø¸Ø§Ù… retry Ø°ÙƒÙŠ Ù…Ø¹ fallback
 * âœ… Caching Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø£Ø¯Ø§Ø¡
 * âœ… Rate limiting Ø°ÙƒÙŠ
 * âœ… Ø§Ø®ØªÙŠØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
 * âœ… ØªØ­Ø³ÙŠÙ† prompts ØªÙ„Ù‚Ø§Ø¦ÙŠ
 * âœ… Analytics Ø´Ø§Ù…Ù„
 * âœ… Ø¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© (Llama, Mistral, CodeLlama, etc.)
 * 
 * @author Oqool AI Team
 * @version 2.0.0
 */

import { Ollama } from 'ollama';
import crypto from 'crypto';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ“Š Types & Interfaces
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
  timestamp?: number;
  metadata?: Record<string, any>;
}

export interface ChatCompletionOptions {
  model?: string;
  maxTokens?: number;
  temperature?: number;
  conversationId?: string;
  priority?: 'speed' | 'quality' | 'cost';
  enableCache?: boolean;
  maxRetries?: number;
}

export interface ToolDefinition {
  name: string;
  description: string;
  parameters: Record<string, any>;
}

export interface ToolCall {
  id: string;
  name: string;
  parameters: any;
}

export interface UnifiedResponse {
  text: string;
  toolCalls?: ToolCall[];
  stopReason?: string;
  metadata?: ResponseMetadata;
}

export interface ResponseMetadata {
  model: string;
  duration: number;
  cost: number;
  tokensUsed: { input: number; output: number };
  cacheHit: boolean;
  retries: number;
  quality: 'low' | 'medium' | 'high';
}

export interface ConversationContext {
  id: string;
  messages: Message[];
  totalDuration: number;
  createdAt: number;
  updatedAt: number;
  metadata?: Record<string, any>;
}

export interface PerformanceStats {
  totalDuration: number;
  totalRequests: number;
  averageDuration: number;
  requestsByModel: Record<string, number>;
  cacheHits: number;
  cacheMisses: number;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ§  Smart Conversation Manager
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SmartConversationManager {
  private conversations: Map<string, ConversationContext> = new Map();
  private readonly MAX_CONTEXT_MESSAGES = 20;
  private readonly MAX_CONTEXT_TOKENS = 100000;
  private readonly CLEANUP_INTERVAL = 3600000;

  constructor() {
    setInterval(() => this.cleanup(), this.CLEANUP_INTERVAL);
  }

  async getOptimizedContext(
    conversationId: string,
    newMessages: Message[]
  ): Promise<Message[]> {
    const context = this.conversations.get(conversationId);

    if (!context) {
      this.createConversation(conversationId, newMessages);
      return newMessages;
    }

    const allMessages = [...context.messages, ...newMessages];
    const optimized = this.smartContextSelection(allMessages);

    context.messages = optimized;
    context.updatedAt = Date.now();
    this.conversations.set(conversationId, context);

    return optimized;
  }

  private smartContextSelection(messages: Message[]): Message[] {
    const systemMessages = messages.filter((m) => m.role === 'system');
    const conversationMessages = messages.filter((m) => m.role !== 'system');
    const recentMessages = conversationMessages.slice(-this.MAX_CONTEXT_MESSAGES);
    const estimatedTokens = this.estimateTokens(recentMessages);

    if (estimatedTokens > this.MAX_CONTEXT_TOKENS) {
      return [
        ...systemMessages,
        ...recentMessages.slice(-Math.floor(this.MAX_CONTEXT_MESSAGES / 2)),
      ];
    }

    return [...systemMessages, ...recentMessages];
  }

  private createConversation(id: string, messages: Message[]): void {
    this.conversations.set(id, {
      id,
      messages,
      totalDuration: 0,
      createdAt: Date.now(),
      updatedAt: Date.now(),
    });
  }

  saveMessage(conversationId: string, message: Message, duration: number): void {
    const context = this.conversations.get(conversationId);
    if (!context) return;

    context.messages.push(message);
    context.totalDuration += duration;
    context.updatedAt = Date.now();
  }

  getStats(conversationId: string): ConversationContext | null {
    return this.conversations.get(conversationId) || null;
  }

  clear(conversationId: string): void {
    this.conversations.delete(conversationId);
  }

  private cleanup(): void {
    const oneDayAgo = Date.now() - 86400000;
    for (const [id, context] of this.conversations.entries()) {
      if (context.updatedAt < oneDayAgo) {
        this.conversations.delete(id);
      }
    }
  }

  private estimateTokens(messages: Message[]): number {
    const totalChars = messages.reduce((sum, m) => sum + m.content.length, 0);
    return Math.ceil(totalChars / 4);
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ› ï¸ Intelligent Tool Optimizer
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class IntelligentToolOptimizer {
  private usageStats: Map<string, number> = new Map();

  optimize(
    tools: ToolDefinition[],
    prompt: string,
    conversationHistory?: Message[]
  ): { optimized: ToolDefinition[]; removed: string[]; reasoning: string } {
    const original = [...tools];
    let optimized = this.removeDuplicates(tools);
    optimized = this.selectRelevantTools(optimized, prompt);
    optimized = this.sortByPriorityAndUsage(optimized, prompt);

    const removed = optimized.slice(10).map((t) => t.name);
    optimized = optimized.slice(0, 10);

    return {
      optimized,
      removed,
      reasoning: `Optimized from ${original.length} to ${optimized.length} tools`,
    };
  }

  private removeDuplicates(tools: ToolDefinition[]): ToolDefinition[] {
    const seen = new Set<string>();
    return tools.filter((tool) => {
      if (seen.has(tool.name)) return false;
      seen.add(tool.name);
      return true;
    });
  }

  private selectRelevantTools(tools: ToolDefinition[], prompt: string): ToolDefinition[] {
    const lowerPrompt = prompt.toLowerCase();
    const toolKeywords: Record<string, string[]> = {
      read_file: ['read', 'show', 'file', 'content', 'Ø§Ù‚Ø±Ø£', 'Ù…Ù„Ù'],
      write_file: ['write', 'create', 'edit', 'save', 'Ø§ÙƒØªØ¨', 'Ø£Ù†Ø´Ø¦'],
      list_directory: ['list', 'folder', 'directory', 'files', 'Ù‚Ø§Ø¦Ù…Ø©', 'Ù…Ø¬Ù„Ø¯'],
      search_files: ['search', 'find', 'locate', 'Ø§Ø¨Ø­Ø«', 'Ø¨Ø­Ø«'],
      execute_command: ['execute', 'run', 'command', 'Ù†ÙØ°', 'Ø£Ù…Ø±'],
    };

    const scored = tools.map((tool) => {
      const keywords = toolKeywords[tool.name] || [];
      const score = keywords.filter((kw) => lowerPrompt.includes(kw)).length;
      return { tool, score };
    });

    const hasRelevance = scored.some((s) => s.score > 0);
    if (!hasRelevance) return tools;

    return scored
      .filter((s) => s.score > 0)
      .sort((a, b) => b.score - a.score)
      .map((s) => s.tool);
  }

  private sortByPriorityAndUsage(tools: ToolDefinition[], prompt: string): ToolDefinition[] {
    const basePriority: Record<string, number> = {
      read_file: 10,
      list_directory: 9,
      search_files: 8,
      write_file: 7,
      execute_command: 6,
    };

    return tools.sort((a, b) => {
      const priorityA = basePriority[a.name] || 5;
      const priorityB = basePriority[b.name] || 5;
      const usageA = this.usageStats.get(a.name) || 0;
      const usageB = this.usageStats.get(b.name) || 0;
      const scoreA = priorityA + usageA * 0.1;
      const scoreB = priorityB + usageB * 0.1;
      return scoreB - scoreA;
    });
  }

  recordUsage(toolName: string): void {
    const current = this.usageStats.get(toolName) || 0;
    this.usageStats.set(toolName, current + 1);
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ“Š Advanced Performance Tracker
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AdvancedPerformanceTracker {
  private totalDuration = 0;
  private requests: Array<{
    timestamp: number;
    model: string;
    inputTokens: number;
    outputTokens: number;
    duration: number;
    cached: boolean;
  }> = [];
  private cacheHits = 0;
  private cacheMisses = 0;

  record(data: {
    model: string;
    inputTokens: number;
    outputTokens: number;
    duration: number;
    cached?: boolean;
  }): void {
    this.requests.push({
      timestamp: Date.now(),
      ...data,
      cached: data.cached || false,
    });

    this.totalDuration += data.duration;

    if (data.cached) {
      this.cacheHits++;
    } else {
      this.cacheMisses++;
    }
  }

  getStats(): PerformanceStats {
    if (this.requests.length === 0) {
      return {
        totalDuration: 0,
        totalRequests: 0,
        averageDuration: 0,
        requestsByModel: {},
        cacheHits: 0,
        cacheMisses: 0,
      };
    }

    const requestsByModel: Record<string, number> = {};

    for (const req of this.requests) {
      requestsByModel[req.model] = (requestsByModel[req.model] || 0) + 1;
    }

    return {
      totalDuration: this.totalDuration,
      totalRequests: this.requests.length,
      averageDuration: this.totalDuration / this.requests.length,
      requestsByModel,
      cacheHits: this.cacheHits,
      cacheMisses: this.cacheMisses,
    };
  }

  displayDetailedReport(): void {
    const stats = this.getStats();
    const cacheHitRate = stats.totalRequests > 0
      ? ((stats.cacheHits / stats.totalRequests) * 100).toFixed(1)
      : '0.0';

    console.log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    console.log('â•‘         ğŸ¦™ ØªÙ‚Ø±ÙŠØ± Ollama - Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…           â•‘');
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log(`â•‘ ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: ${stats.totalRequests.toString().padEnd(34)} â•‘`);
    console.log(`â•‘ â±ï¸  Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: ${(stats.totalDuration / 1000).toFixed(2)}s${' '.repeat(32 - (stats.totalDuration / 1000).toFixed(2).length)} â•‘`);
    console.log(`â•‘ ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¯Ø©: ${stats.averageDuration.toFixed(0)}ms${' '.repeat(36 - stats.averageDuration.toFixed(0).length)} â•‘`);
    console.log(`â•‘ ğŸ’¾ Cache Hits: ${stats.cacheHits} (${cacheHitRate}%)${' '.repeat(32 - stats.cacheHits.toString().length - cacheHitRate.length)} â•‘`);
    console.log(`â•‘ ğŸ’µ Ø§Ù„ØªÙƒÙ„ÙØ©: $0.00 (Ù…Ø¬Ø§Ù†ÙŠ!)${' '.repeat(30)} â•‘`);
    console.log('â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£');
    console.log('â•‘ ğŸ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:                             â•‘');
    for (const [model, count] of Object.entries(stats.requestsByModel)) {
      const modelShort = model.split(':')[0];
      console.log(`â•‘   â€¢ ${modelShort}: ${count} Ø·Ù„Ø¨${' '.repeat(38 - modelShort.length - count.toString().length)} â•‘`);
    }
    console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
  }

  reset(): void {
    this.totalDuration = 0;
    this.requests = [];
    this.cacheHits = 0;
    this.cacheMisses = 0;
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ’¾ Smart Cache System
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SmartCache {
  private cache: Map<string, { response: string; timestamp: number; hits: number }> = new Map();
  private readonly MAX_CACHE_SIZE = 100;
  private readonly CACHE_TTL = 3600000;

  private generateKey(messages: Message[], options: any): string {
    const content = messages.map((m) => `${m.role}:${m.content}`).join('|');
    const optsStr = JSON.stringify({ model: options.model, temp: options.temperature });
    return crypto.createHash('md5').update(content + optsStr).digest('hex');
  }

  get(messages: Message[], options: any): string | null {
    this.cleanup();
    const key = this.generateKey(messages, options);
    const cached = this.cache.get(key);
    if (!cached) return null;
    cached.hits++;
    this.cache.set(key, cached);
    return cached.response;
  }

  set(messages: Message[], options: any, response: string): void {
    const key = this.generateKey(messages, options);
    if (this.cache.size >= this.MAX_CACHE_SIZE) {
      this.evictLeastUsed();
    }
    this.cache.set(key, { response, timestamp: Date.now(), hits: 0 });
  }

  private evictLeastUsed(): void {
    let minHits = Infinity;
    let keyToRemove = '';
    for (const [key, value] of this.cache.entries()) {
      if (value.hits < minHits) {
        minHits = value.hits;
        keyToRemove = key;
      }
    }
    if (keyToRemove) this.cache.delete(keyToRemove);
  }

  private cleanup(): void {
    const now = Date.now();
    for (const [key, value] of this.cache.entries()) {
      if (now - value.timestamp > this.CACHE_TTL) {
        this.cache.delete(key);
      }
    }
  }

  clear(): void {
    this.cache.clear();
  }

  getStats(): { size: number; hitRate: number } {
    const totalHits = Array.from(this.cache.values()).reduce((sum, v) => sum + v.hits, 0);
    return {
      size: this.cache.size,
      hitRate: this.cache.size > 0 ? totalHits / this.cache.size : 0,
    };
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸ¯ Smart Model Selector
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SmartModelSelector {
  selectOptimalModel(
    prompt: string,
    options: ChatCompletionOptions,
    conversationLength?: number,
    availableModels?: string[]
  ): string {
    if (options.model) return options.model;

    // Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¹Ø±Ù Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    if (!availableModels || availableModels.length === 0) {
      return 'llama3.2:latest';
    }

    switch (options.priority) {
      case 'speed':
        return this.selectFastest(availableModels);
      case 'quality':
        return this.selectBest(availableModels, prompt);
      case 'cost':
        return this.selectSmallest(availableModels); // Ù…Ø¬Ø§Ù†ÙŠ Ù„ÙƒÙ† Ø£Ø³Ø±Ø¹
      default:
        return this.autoSelect(prompt, conversationLength || 0, availableModels);
    }
  }

  private selectFastest(models: string[]): string {
    // Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµØºÙŠØ±Ø© Ø£Ø³Ø±Ø¹
    const fastModels = ['llama3.2:1b', 'phi3:mini', 'qwen2:1.5b'];
    for (const fast of fastModels) {
      const found = models.find((m) => m.includes(fast.split(':')[0]));
      if (found) return found;
    }
    return models[0];
  }

  private selectBest(models: string[], prompt: string): string {
    // Ù„Ù„ÙƒÙˆØ¯: CodeLlama
    if (this.isCodeRelated(prompt)) {
      const codeLlama = models.find((m) => m.includes('codellama'));
      if (codeLlama) return codeLlama;
    }

    // Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©: Ø£ÙƒØ¨Ø± Ù†Ù…ÙˆØ°Ø¬
    const largeModels = ['llama3.1:70b', 'llama3.1:405b', 'mixtral:8x22b'];
    for (const large of largeModels) {
      const found = models.find((m) => m.includes(large.split(':')[0]));
      if (found) return found;
    }

    return models[0];
  }

  private selectSmallest(models: string[]): string {
    const smallModels = ['llama3.2:1b', 'phi3:mini', 'qwen2:1.5b'];
    for (const small of smallModels) {
      const found = models.find((m) => m.includes(small.split(':')[0]));
      if (found) return found;
    }
    return models[0];
  }

  private autoSelect(prompt: string, conversationLength: number, models: string[]): string {
    const promptLength = prompt.length;

    // Ù…Ù‡Ù…Ø© Ø¨Ø³ÙŠØ·Ø© = Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ±
    if (promptLength < 200 && conversationLength < 5) {
      return this.selectSmallest(models);
    }

    // ÙƒÙˆØ¯ = CodeLlama
    if (this.isCodeRelated(prompt)) {
      const codeLlama = models.find((m) => m.includes('codellama'));
      if (codeLlama) return codeLlama;
    }

    // Ù…Ø¹Ù‚Ø¯ = Ù†Ù…ÙˆØ°Ø¬ ÙƒØ¨ÙŠØ±
    if (this.isComplex(prompt)) {
      return this.selectBest(models, prompt);
    }

    // Ù…ØªÙˆØ³Ø· = llama3.2 Ø£Ùˆ llama3.1
    const defaultModels = ['llama3.2:latest', 'llama3.1:8b', 'mistral:latest'];
    for (const def of defaultModels) {
      const found = models.find((m) => m.includes(def.split(':')[0]));
      if (found) return found;
    }

    return models[0];
  }

  private isCodeRelated(prompt: string): boolean {
    const codeKeywords = [
      'code', 'function', 'class', 'debug', 'error', 'ÙƒÙˆØ¯', 'Ø¨Ø±Ù…Ø¬Ø©',
      'typescript', 'python', 'javascript', 'react', 'api'
    ];
    const lower = prompt.toLowerCase();
    return codeKeywords.some((kw) => lower.includes(kw));
  }

  private isComplex(prompt: string): boolean {
    const complexKeywords = [
      'complex', 'advanced', 'architecture', 'comprehensive',
      'Ù…Ø¹Ù‚Ø¯', 'Ù…ØªÙ‚Ø¯Ù…', 'Ù…Ø¹Ù…Ø§Ø±ÙŠ', 'Ø´Ø§Ù…Ù„'
    ];
    const lower = prompt.toLowerCase();
    return complexKeywords.some((kw) => lower.includes(kw));
  }
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// ğŸš€ Main Ollama Service Class
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export class OllamaService {
  private client: Ollama;
  private conversationManager: SmartConversationManager;
  private toolOptimizer: IntelligentToolOptimizer;
  private performanceTracker: AdvancedPerformanceTracker;
  private cache: SmartCache;
  private modelSelector: SmartModelSelector;
  private availableModels: string[] = [];

  private requestQueue: Array<() => Promise<any>> = [];
  private processingQueue = false;
  private readonly MAX_REQUESTS_PER_MINUTE = 100; // Ù…Ø­Ù„ÙŠ = unlimited ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
  private requestsThisMinute = 0;

  constructor(host?: string) {
    this.client = new Ollama({ host: host || 'http://localhost:11434' });
    this.conversationManager = new SmartConversationManager();
    this.toolOptimizer = new IntelligentToolOptimizer();
    this.performanceTracker = new AdvancedPerformanceTracker();
    this.cache = new SmartCache();
    this.modelSelector = new SmartModelSelector();

    // ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
    this.loadAvailableModels();

    setInterval(() => {
      this.requestsThisMinute = 0;
    }, 60000);
  }

  private async loadAvailableModels(): Promise<void> {
    try {
      const response = await this.client.list();
      this.availableModels = response.models.map((m: any) => m.name);
    } catch (error) {
      console.warn('âš ï¸ ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©');
      this.availableModels = ['llama3.2:latest'];
    }
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ’¬ Chat Completion (Basic)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  async chatCompletion(
    messages: Message[],
    options: ChatCompletionOptions = {}
  ): Promise<string> {
    return this.executeWithRateLimit(async () => {
      const startTime = Date.now();

      const model = this.modelSelector.selectOptimalModel(
        messages[messages.length - 1]?.content || '',
        options,
        messages.length,
        this.availableModels
      );

      if (options.enableCache !== false) {
        const cached = this.cache.get(messages, { ...options, model });
        if (cached) {
          console.log('âœ… Cache hit!');
          return cached;
        }
      }

      let processedMessages = messages;
      if (options.conversationId) {
        processedMessages = await this.conversationManager.getOptimizedContext(
          options.conversationId,
          messages
        );
      }

      try {
        const response = await this.client.chat({
          model,
          messages: processedMessages.map((m) => ({
            role: m.role,
            content: m.content,
          })),
          options: {
            temperature: options.temperature || 0.7,
            num_predict: options.maxTokens || 4096,
          },
        });

        const content = response.message.content;
        const duration = Date.now() - startTime;

        // ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù€ tokens
        const inputTokens = this.estimateTokens(
          processedMessages.map((m) => m.content).join(' ')
        );
        const outputTokens = this.estimateTokens(content);

        this.performanceTracker.record({
          model,
          inputTokens,
          outputTokens,
          duration,
          cached: false,
        });

        if (options.conversationId) {
          this.conversationManager.saveMessage(
            options.conversationId,
            { role: 'assistant', content },
            duration
          );
        }

        if (options.enableCache !== false) {
          this.cache.set(messages, { ...options, model }, content);
        }

        return content;
      } catch (error: any) {
        return this.handleError(error, messages, options);
      }
    });
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ› ï¸ Chat with Tools (Advanced)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  async chatWithTools(
    messages: Message[],
    tools: ToolDefinition[],
    options: ChatCompletionOptions = {}
  ): Promise<UnifiedResponse> {
    return this.executeWithRateLimit(async () => {
      const startTime = Date.now();
      let totalDuration = 0;
      let totalInputTokens = 0;
      let totalOutputTokens = 0;
      let iterations = 0;
      const maxIterations = 15;

      const model = this.modelSelector.selectOptimalModel(
        messages[messages.length - 1]?.content || '',
        options,
        messages.length,
        this.availableModels
      );

      const prompt = messages[messages.length - 1]?.content || '';
      const optimizationResult = this.toolOptimizer.optimize(tools, prompt, messages);

      console.log(`ğŸ› ï¸  ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯ÙˆØ§Øª: ${tools.length} â†’ ${optimizationResult.optimized.length}`);

      let processedMessages = messages;
      if (options.conversationId) {
        processedMessages = await this.conversationManager.getOptimizedContext(
          options.conversationId,
          messages
        );
      }

      const ollamaTools = this.adaptToolsForOllama(optimizationResult.optimized);
      let conversationMessages = [...processedMessages];
      let finalResponse = '';

      while (iterations < maxIterations) {
        try {
          const iterStart = Date.now();

          const response = await this.client.chat({
            model,
            messages: conversationMessages.map((m) => ({
              role: m.role,
              content: m.content,
            })),
            tools: ollamaTools,
            options: {
              temperature: options.temperature || 0.7,
              num_predict: options.maxTokens || 4096,
            },
          });

          const iterDuration = Date.now() - iterStart;
          totalDuration += iterDuration;

          const inputTokens = this.estimateTokens(conversationMessages.map((m) => m.content).join(' '));
          const outputTokens = this.estimateTokens(response.message.content);

          totalInputTokens += inputTokens;
          totalOutputTokens += outputTokens;
          iterations++;

          if (response.message.content) {
            finalResponse += response.message.content;
          }

          if (response.message.tool_calls && response.message.tool_calls.length > 0) {
            conversationMessages.push({
              role: 'assistant',
              content: response.message.content,
            });

            for (const toolCall of response.message.tool_calls) {
              console.log(`ğŸ”§ ØªÙ†ÙÙŠØ°: ${toolCall.function.name}`);

              const result = await this.executeToolSafely(
                toolCall.function.name,
                toolCall.function.arguments
              );

              this.toolOptimizer.recordUsage(toolCall.function.name);

              conversationMessages.push({
                role: 'system',
                content: `Tool result: ${JSON.stringify(result)}`,
              });
            }
          } else {
            break;
          }
        } catch (error: any) {
          console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ iteration ${iterations}:`, error.message);
          break;
        }
      }

      const duration = Date.now() - startTime;

      this.performanceTracker.record({
        model,
        inputTokens: totalInputTokens,
        outputTokens: totalOutputTokens,
        duration,
        cached: false,
      });

      if (options.conversationId) {
        this.conversationManager.saveMessage(
          options.conversationId,
          { role: 'assistant', content: finalResponse },
          duration
        );
      }

      return {
        text: finalResponse,
        stopReason: iterations >= maxIterations ? 'max_iterations' : 'stop',
        metadata: {
          model,
          duration,
          cost: 0, // Ù…Ø¬Ø§Ù†ÙŠ!
          tokensUsed: { input: totalInputTokens, output: totalOutputTokens },
          cacheHit: false,
          retries: 0,
          quality: this.assessQuality(model),
        },
      };
    });
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸŒŠ Streaming Support
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  async *chatCompletionStream(
    messages: Message[],
    options: ChatCompletionOptions = {}
  ): AsyncGenerator<string, void, unknown> {
    const model = this.modelSelector.selectOptimalModel(
      messages[messages.length - 1]?.content || '',
      options,
      messages.length,
      this.availableModels
    );

    try {
      const stream = await this.client.chat({
        model,
        messages: messages.map((m) => ({ role: m.role, content: m.content })),
        stream: true,
        options: {
          temperature: options.temperature || 0.7,
          num_predict: options.maxTokens || 4096,
        },
      });

      for await (const chunk of stream) {
        if (chunk.message?.content) {
          yield chunk.message.content;
        }
      }
    } catch (error: any) {
      console.error('Ollama Stream Error:', error);
      throw error;
    }
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ”§ Helper Methods
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  private adaptToolsForOllama(tools: ToolDefinition[]): any[] {
    return tools.map((tool) => ({
      type: 'function',
      function: {
        name: tool.name,
        description: tool.description,
        parameters: {
          type: 'object',
          properties: tool.parameters,
          required: Object.keys(tool.parameters),
        },
      },
    }));
  }

  private async executeToolSafely(name: string, params: any): Promise<any> {
    try {
      const { executeTool } = await import('../core/tools.js');
      return await executeTool(name, params);
    } catch (error: any) {
      return { success: false, error: error.message };
    }
  }

  private estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  private async handleError(
    error: any,
    messages: Message[],
    options: ChatCompletionOptions,
    retryCount: number = 0
  ): Promise<string> {
    const maxRetries = options.maxRetries || 3;

    console.error(`âŒ Ø®Ø·Ø£ Ollama (Ù…Ø­Ø§ÙˆÙ„Ø© ${retryCount + 1}/${maxRetries}):`, error.message);

    if (error.message?.includes('connection') || retryCount >= maxRetries) {
      throw new Error(this.enhanceError(error));
    }

    await this.sleep(1000);
    return this.chatCompletion(messages, { ...options, maxRetries: maxRetries - retryCount - 1 });
  }

  private enhanceError(error: any): string {
    const errorMsg = error.message || '';

    if (errorMsg.includes('connection') || errorMsg.includes('ECONNREFUSED')) {
      return 'Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Ollama. ØªØ£ÙƒØ¯ Ø£Ù† Ollama ÙŠØ¹Ù…Ù„ (ollama serve)';
    }
    if (errorMsg.includes('not found') || errorMsg.includes('model')) {
      return 'Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù‚Ù… Ø¨ØªÙ†Ø²ÙŠÙ„Ù‡ Ø£ÙˆÙ„Ø§Ù‹ (ollama pull model-name)';
    }

    return errorMsg || 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ';
  }

  private assessQuality(model: string): 'low' | 'medium' | 'high' {
    if (model.includes('70b') || model.includes('405b')) return 'high';
    if (model.includes('13b') || model.includes('8b')) return 'medium';
    return 'low';
  }

  private async executeWithRateLimit<T>(fn: () => Promise<T>): Promise<T> {
    if (this.requestsThisMinute >= this.MAX_REQUESTS_PER_MINUTE) {
      console.warn('âš ï¸ ÙˆØµÙ„Øª Ù„Ø­Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§ØªØŒ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...');
      await this.sleep(1000);
      return this.executeWithRateLimit(fn);
    }

    this.requestsThisMinute++;
    return fn();
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ“Š Public API - Stats & Management
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  getPerformanceStats(): PerformanceStats {
    return this.performanceTracker.getStats();
  }

  displayDetailedReport(): void {
    this.performanceTracker.displayDetailedReport();
  }

  getConversationStats(conversationId: string): ConversationContext | null {
    return this.conversationManager.getStats(conversationId);
  }

  clearConversation(conversationId: string): void {
    this.conversationManager.clear(conversationId);
  }

  clearCache(): void {
    this.cache.clear();
  }

  getCacheStats(): { size: number; hitRate: number } {
    return this.cache.getStats();
  }

  resetPerformanceTracking(): void {
    this.performanceTracker.reset();
  }

  async validateConnection(): Promise<boolean> {
    try {
      await this.client.list();
      return true;
    } catch {
      return false;
    }
  }

  async getAvailableModels() {
    try {
      const response = await this.client.list();
      return response.models.map((m: any) => ({
        id: m.name,
        name: m.name,
        size: m.size,
        modified: m.modified_at,
        supportsTools: this.modelSupportsTools(m.name),
      }));
    } catch (error) {
      console.error('Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:', error);
      return [];
    }
  }

  private modelSupportsTools(modelName: string): boolean {
    // Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Tools
    const toolsSupport = ['llama3.1', 'llama3.2', 'mistral', 'mixtral'];
    return toolsSupport.some((m) => modelName.includes(m));
  }

  getModelInfo() {
    return {
      name: 'Ollama (Local)',
      version: '2.0.0',
      features: [
        'Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø°ÙƒÙŠØ© Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø³ÙŠØ§Ù‚',
        'Ø¯Ø¹Ù… Ø£Ø¯ÙˆØ§Øª Ù…ØªÙ‚Ø¯Ù… (Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)',
        'ØªØªØ¨Ø¹ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø£Ø¯Ø§Ø¡',
        'ØªØ­Ø³ÙŠÙ† ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø£Ø¯ÙˆØ§Øª',
        'Ù†Ø¸Ø§Ù… cache Ø°ÙƒÙŠ',
        'Ø§Ø®ØªÙŠØ§Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨',
        'Ù…Ø¬Ø§Ù†ÙŠ 100% ÙˆÙ…Ø­Ù„ÙŠ',
        'Ø®ØµÙˆØµÙŠØ© ÙƒØ§Ù…Ù„Ø©',
      ],
      defaultModel: 'llama3.2:latest',
      supportsTools: true,
      supportsStreaming: true,
      supportsConversations: true,
      cost: 0,
      privacy: 'full',
    };
  }
}

export default OllamaService;
